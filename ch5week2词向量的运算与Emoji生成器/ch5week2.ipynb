{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.è¯å‘é‡è¿ç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import w2v_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
      " -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
      "  0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
      "  0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
      "  0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
      " -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
      " -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
      "  0.67204 ]\n"
     ]
    }
   ],
   "source": [
    "words, word_to_vec_map = w2v_utils.read_glove_vecs('data/glove.6B.50d.txt')\n",
    "\n",
    "print(word_to_vec_map['hello'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 ä½™å¼¦ç›¸ä¼¼åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    uä¸vçš„ä½™å¼¦ç›¸ä¼¼åº¦åæ˜ äº†uä¸vçš„ç›¸ä¼¼ç¨‹åº¦\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        u -- ç»´åº¦ä¸º(n,)çš„è¯å‘é‡\n",
    "        v -- ç»´åº¦ä¸º(n,)çš„è¯å‘é‡\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        cosine_similarity -- uå’Œvä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚\n",
    "    \"\"\"\n",
    "    dot = np.dot(u, v)\n",
    "    \n",
    "    norm_u = np.sqrt(np.sum(np.power(u, 2)))\n",
    "    norm_v = np.sqrt(np.sum(np.power(v, 2)))\n",
    "    \n",
    "    cosine_similarity = np.divide(dot, norm_u * norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(father, mother) =  0.8909038442893615\n",
      "cosine_similarity(ball, crocodile) =  0.27439246261379424\n",
      "cosine_similarity(france - paris, rome - italy) =  -0.6751479308174202\n"
     ]
    }
   ],
   "source": [
    "father = word_to_vec_map[\"father\"]\n",
    "mother = word_to_vec_map[\"mother\"]\n",
    "ball = word_to_vec_map[\"ball\"]\n",
    "crocodile = word_to_vec_map[\"crocodile\"]\n",
    "france = word_to_vec_map[\"france\"]\n",
    "italy = word_to_vec_map[\"italy\"]\n",
    "paris = word_to_vec_map[\"paris\"]\n",
    "rome = word_to_vec_map[\"rome\"]\n",
    "\n",
    "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n",
    "print(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 è¯ç±»ç±»æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    è§£å†³â€œAä¸Bç›¸æ¯”å°±ç±»ä¼¼äºCä¸____ç›¸æ¯”ä¸€æ ·â€ä¹‹ç±»çš„é—®é¢˜\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        word_a -- ä¸€ä¸ªå­—ç¬¦ä¸²ç±»å‹çš„è¯\n",
    "        word_b -- ä¸€ä¸ªå­—ç¬¦ä¸²ç±»å‹çš„è¯\n",
    "        word_c -- ä¸€ä¸ªå­—ç¬¦ä¸²ç±»å‹çš„è¯\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹ï¼Œå•è¯åˆ°GloVeå‘é‡çš„æ˜ å°„\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        best_word -- æ»¡è¶³(v_b - v_a) æœ€æ¥è¿‘ (v_best_word - v_c) çš„è¯\n",
    "    \"\"\"\n",
    "    # æŠŠå•è¯è½¬æ¢ä¸ºå°å†™\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    e_a = word_to_vec_map[word_a]\n",
    "    e_b = word_to_vec_map[word_b]\n",
    "    e_c = word_to_vec_map[word_c]\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    max_similarity = -100\n",
    "    best_word = None\n",
    "    \n",
    "    for word in words:\n",
    "        if word in [word_a,word_b,word_c]:\n",
    "            continue\n",
    "            \n",
    "        cos_similarity = cosine_similarity(e_b-e_a, word_to_vec_map[word]-e_c)\n",
    "        if cos_similarity > max_similarity:\n",
    "            max_similarity = cos_similarity\n",
    "            best_word = word\n",
    "    \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian <====> spain -> spanish\n",
      "india -> delhi <====> japan -> tokyo\n",
      "man -> woman <====> boy -> girl\n",
      "small -> smaller <====> large -> larger\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} <====> {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 å»é™¤è¯å‘é‡çš„åè§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¶ˆé™¤ä¸æ€§åˆ«è¯æ— å…³è¯æ±‡åå·®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.087144    0.2182     -0.40986    -0.03922    -0.1032      0.94165\n",
      " -0.06042     0.32988     0.46144    -0.35962     0.31102    -0.86824\n",
      "  0.96006     0.01073     0.24337     0.08193    -1.02722    -0.21122\n",
      "  0.695044   -0.00222     0.29106     0.5053     -0.099454    0.40445\n",
      "  0.30181     0.1355     -0.0606     -0.07131    -0.19245    -0.06115\n",
      " -0.3204      0.07165    -0.13337    -0.25068714 -0.14293    -0.224957\n",
      " -0.149       0.048882    0.12191    -0.27362    -0.165476   -0.20426\n",
      "  0.54376    -0.271425   -0.10245    -0.32108     0.2516     -0.33455\n",
      " -0.04371     0.01258   ]\n",
      "lipstick 0.2769191625638266\n",
      "guns -0.1888485567898898\n",
      "science -0.060829065409296994\n",
      "arts 0.008189312385880328\n",
      "literature 0.06472504433459927\n",
      "warrior -0.20920164641125288\n",
      "doctor 0.11895289410935041\n",
      "tree -0.07089399175478091\n",
      "receptionist 0.33077941750593737\n",
      "technology -0.13193732447554296\n",
      "fashion 0.03563894625772699\n",
      "teacher 0.17920923431825664\n",
      "engineer -0.08039280494524072\n",
      "pilot 0.0010764498991916787\n",
      "computer -0.10330358873850498\n",
      "singer 0.1850051813649629\n"
     ]
    }
   ],
   "source": [
    "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "print(g)\n",
    "\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word, g, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    é€šè¿‡å°†â€œwordâ€æŠ•å½±åˆ°ä¸åç½®è½´æ­£äº¤çš„ç©ºé—´ä¸Šï¼Œæ¶ˆé™¤äº†â€œwordâ€çš„åå·®ã€‚\n",
    "    è¯¥å‡½æ•°ç¡®ä¿â€œwordâ€åœ¨æ€§åˆ«çš„å­ç©ºé—´ä¸­çš„å€¼ä¸º0\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        word -- å¾…æ¶ˆé™¤åå·®çš„å­—ç¬¦ä¸²\n",
    "        g -- ç»´åº¦ä¸º(50,)ï¼Œå¯¹åº”äºåç½®è½´ï¼ˆå¦‚æ€§åˆ«ï¼‰\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹ï¼Œå•è¯åˆ°GloVeå‘é‡çš„æ˜ å°„\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        e_debiased -- æ¶ˆé™¤äº†åå·®çš„å‘é‡ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    # æ ¹æ®wordé€‰æ‹©å¯¹åº”çš„è¯å‘é‡\n",
    "    e = word_to_vec_map[word]\n",
    "    \n",
    "    # æ ¹æ®å…¬å¼2è®¡ç®—e_biascomponent\n",
    "    e_biascomponent = np.divide(np.dot(e, g), np.square(np.linalg.norm(g))) * g\n",
    "    \n",
    "    # æ ¹æ®å…¬å¼3è®¡ç®—e_debiased\n",
    "    e_debiased = e - e_biascomponent\n",
    "    \n",
    "    return e_debiased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å»åå·®å‰receptionistä¸gçš„ä½™å¼¦ç›¸ä¼¼åº¦ä¸ºï¼š0.33077941750593737\n",
      "å»åå·®åreceptionistä¸gçš„ä½™å¼¦ç›¸ä¼¼åº¦ä¸ºï¼š1.1682064664487028e-17\n"
     ]
    }
   ],
   "source": [
    "e = \"receptionist\"\n",
    "print(\"å»åå·®å‰{0}ä¸gçš„ä½™å¼¦ç›¸ä¼¼åº¦ä¸ºï¼š{1}\".format(e, cosine_similarity(word_to_vec_map[\"receptionist\"], g)))\n",
    "\n",
    "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
    "print(\"å»åå·®å{0}ä¸gçš„ä½™å¼¦ç›¸ä¼¼åº¦ä¸ºï¼š{1}\".format(e, cosine_similarity(e_debiased, g)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ€§åˆ«è¯çš„å‡è¡¡ç®—æ³•ï¼ˆç•¥ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 0.]\n",
      "[0. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([2,2])\n",
    "b = np.array([1,0])\n",
    "e_bias = np.divide(np.dot(a, b), np.square(np.linalg.norm(b))) * b\n",
    "print(e_bias)\n",
    "print(a - e_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Emoijè¡¨æƒ…ç”Ÿæˆå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emo_utils\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 åŸºå‡†æ¨¡å‹ - V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = emo_utils.read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = emo_utils.read_csv('data/test.csv')\n",
    "\n",
    "maxLen = len(max(X_train, key=len).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss you so much â¤ï¸\n"
     ]
    }
   ],
   "source": [
    "index  = 3\n",
    "print(X_train[index], emo_utils.label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½¬æ¢æˆç‹¬çƒ­ç¼–ç \n",
    "Y_oh_train = emo_utils.convert_to_one_hot(Y_train, C=5)\n",
    "Y_oh_test = emo_utils.convert_to_one_hot(Y_test, C=5)\n",
    "\n",
    "# è¯»å–gloveè¯åµŒå…¥\n",
    "word_to_index, index_to_word, word_to_vec_map = emo_utils.read_glove_vecs('data/glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å•è¯cucumberå¯¹åº”çš„ç´¢å¼•æ˜¯ï¼š113317\n",
      "ç´¢å¼•113317å¯¹åº”çš„å•è¯æ˜¯ï¼šcucumber\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "index = 113317\n",
    "print(\"å•è¯{0}å¯¹åº”çš„ç´¢å¼•æ˜¯ï¼š{1}\".format(word, word_to_index[word]))\n",
    "print(\"ç´¢å¼•{0}å¯¹åº”çš„å•è¯æ˜¯ï¼š{1}\".format(index, index_to_word[index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    å°†å¥å­è½¬æ¢ä¸ºå•è¯åˆ—è¡¨ï¼Œæå–å…¶GloVeå‘é‡ï¼Œç„¶åå°†å…¶å¹³å‡ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        sentence -- å­—ç¬¦ä¸²ç±»å‹ï¼Œä»Xä¸­è·å–çš„æ ·æœ¬ã€‚\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹ï¼Œå•è¯æ˜ å°„åˆ°50ç»´çš„å‘é‡çš„å­—å…¸\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        avg -- å¯¹å¥å­çš„å‡å€¼ç¼–ç ï¼Œç»´åº¦ä¸º(50,)\n",
    "    \"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    vec = np.zeros((50,))\n",
    "    \n",
    "    for word in words:\n",
    "        vec += word_to_vec_map[word]\n",
    "    \n",
    "    avg = np.divide(vec, len(words))\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate=0.01, num_iterations=400):\n",
    "    \"\"\"\n",
    "    åœ¨numpyä¸­è®­ç»ƒè¯å‘é‡æ¨¡å‹ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        X -- è¾“å…¥çš„å­—ç¬¦ä¸²ç±»å‹çš„æ•°æ®ï¼Œç»´åº¦ä¸º(m, 1)ã€‚\n",
    "        Y -- å¯¹åº”çš„æ ‡ç­¾ï¼Œ0-7çš„æ•°ç»„ï¼Œç»´åº¦ä¸º(m, 1)ã€‚\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹çš„å•è¯åˆ°50ç»´è¯å‘é‡çš„æ˜ å°„ã€‚\n",
    "        learning_rate -- å­¦ä¹ ç‡.\n",
    "        num_iterations -- è¿­ä»£æ¬¡æ•°ã€‚\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        pred -- é¢„æµ‹çš„å‘é‡ï¼Œç»´åº¦ä¸º(m, 1)ã€‚\n",
    "        W -- æƒé‡å‚æ•°ï¼Œç»´åº¦ä¸º(n_y, n_h)ã€‚\n",
    "        b -- åç½®å‚æ•°ï¼Œç»´åº¦ä¸º(n_y,)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    n_y = 5\n",
    "    n_h = 50\n",
    "    \n",
    "    # ä½¿ç”¨Xavieråˆå§‹åŒ–å‚æ•°\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    Y_oh = emo_utils.convert_to_one_hot(Y, C=n_y)\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        for i in range(m):\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "            \n",
    "            z = np.dot(W, avg) + b\n",
    "            a = emo_utils.softmax(z)\n",
    "            \n",
    "            # è®¡ç®—æŸå¤±\n",
    "            cost = - np.sum(Y_oh[i] * np.log(a))\n",
    "            \n",
    "            # è®¡ç®—æ¢¯åº¦\n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "            \n",
    "            # æ›´æ–°å‚æ•°\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "        if t % 100 == 0:\n",
    "            print(\"{t}è½®æŸå¤±ï¼š{cost}\".format(t=t, cost=cost))\n",
    "            pred = emo_utils.predict(X, Y, W, b, word_to_vec_map)\n",
    "            \n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132,)\n",
      "(132,)\n",
      "(132, 5)\n",
      "never talk to me again\n",
      "<class 'numpy.ndarray'>\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(X_train[0])\n",
    "print(type(X_train))\n",
    "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
    "print(Y.shape)\n",
    "\n",
    "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
    " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
    " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
    " 'You totally deserve this prize', 'Let us go play football',\n",
    " 'Are you down for football this afternoon', 'Work hard play harder',\n",
    " 'It is suprising how people can be dumb sometimes',\n",
    " 'I am very disappointed','It is the best day in my life',\n",
    " 'I think I will end up alone','My life is so boring','Good job',\n",
    " 'Great so awesome'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ & æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0è½®æŸå¤±ï¼š1.9520498812810072\n",
      "Accuracy: 0.3484848484848485\n",
      "100è½®æŸå¤±ï¼š0.07971818726014807\n",
      "Accuracy: 0.9318181818181818\n",
      "200è½®æŸå¤±ï¼š0.04456369243681402\n",
      "Accuracy: 0.9545454545454546\n",
      "300è½®æŸå¤±ï¼š0.03432267378786059\n",
      "Accuracy: 0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====è®­ç»ƒé›†====\n",
      "Accuracy: 0.9772727272727273\n",
      "=====æµ‹è¯•é›†====\n",
      "Accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(\"=====è®­ç»ƒé›†====\")\n",
    "pred_train = emo_utils.predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print(\"=====æµ‹è¯•é›†====\")\n",
    "pred_test = emo_utils.predict(X_test, Y_test, W, b, word_to_vec_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "\n",
      "i adore you â¤ï¸\n",
      "i love you â¤ï¸\n",
      "funny lol ğŸ˜„\n",
      "lets play with a ball âš¾\n",
      "food is ready ğŸ´\n",
      "you are not happy â¤ï¸\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"you are not happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = emo_utils.predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "emo_utils.print_predictions(X_my_sentences, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t â¤ï¸ \t âš¾ \t ğŸ˜„ \t ğŸ˜ \t ğŸ´\n",
      "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
      "Actual                                 \n",
      "0            6    0    0    1    0    7\n",
      "1            0    8    0    0    0    8\n",
      "2            2    0   16    0    0   18\n",
      "3            1    1    2   12    0   16\n",
      "4            0    0    1    0    6    7\n",
      "All          9    9   19   13    6   56\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\" \\t {0} \\t {1} \\t {2} \\t {3} \\t {4}\".format(emo_utils.label_to_emoji(0), emo_utils.label_to_emoji(1), \\\n",
    "                                                 emo_utils.label_to_emoji(2), emo_utils.label_to_emoji(3), \\\n",
    "                                                 emo_utils.label_to_emoji(4)))\n",
    "import pandas as pd\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "emo_utils.plot_confusion_matrix(Y_test, pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 åœ¨kerasä¸­ä½¿ç”¨lstm - V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "np.random.seed(1)\n",
    "from keras.initializers import glorot_uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    è¾“å…¥çš„æ˜¯Xï¼ˆå­—ç¬¦ä¸²ç±»å‹çš„å¥å­çš„æ•°ç»„ï¼‰ï¼Œå†è½¬åŒ–ä¸ºå¯¹åº”çš„å¥å­åˆ—è¡¨ï¼Œ\n",
    "    è¾“å‡ºçš„æ˜¯èƒ½å¤Ÿè®©Embedding()å‡½æ•°æ¥å—çš„åˆ—è¡¨æˆ–çŸ©é˜µï¼ˆå‚è§å›¾4ï¼‰ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        X -- å¥å­æ•°ç»„ï¼Œç»´åº¦ä¸º(m, 1)\n",
    "        word_to_index -- å­—å…¸ç±»å‹çš„å•è¯åˆ°ç´¢å¼•çš„æ˜ å°„\n",
    "        max_len -- æœ€å¤§å¥å­çš„é•¿åº¦ï¼Œæ•°æ®é›†ä¸­æ‰€æœ‰çš„å¥å­çš„é•¿åº¦éƒ½ä¸ä¼šè¶…è¿‡å®ƒã€‚\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        X_indices -- å¯¹åº”äºXä¸­çš„å•è¯ç´¢å¼•æ•°ç»„ï¼Œç»´åº¦ä¸º(m, max_len)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):\n",
    "        words = X[i].lower().split()\n",
    "        j = 0\n",
    "        for w in words:\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j += 1\n",
    "            if j >= max_len:\n",
    "                break;\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices = [[155345. 225122.      0.      0.      0.]\n",
      " [220930. 286375.  69714.      0.      0.]\n",
      " [151204. 192973. 302254. 151349. 394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ„å»ºemdeddingå±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºKeras Embedding()å±‚ï¼ŒåŠ è½½å·²ç»è®­ç»ƒå¥½äº†çš„50ç»´GloVeå‘é‡\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹çš„å•è¯ä¸è¯åµŒå…¥çš„æ˜ å°„\n",
    "        word_to_index -- å­—å…¸ç±»å‹çš„å•è¯åˆ°è¯æ±‡è¡¨ï¼ˆ400,001ä¸ªå•è¯ï¼‰çš„ç´¢å¼•çš„æ˜ å°„ã€‚\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        embedding_layer() -- è®­ç»ƒå¥½äº†çš„Kerasçš„å®ä½“å±‚ã€‚\n",
    "    \"\"\"\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"hello\"].shape[0]\n",
    "    print(\"embedding dim:\", emb_dim)\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word,index in word_to_index.items():\n",
    "        emb_matrix[index,:] = word_to_vec_map[word]\n",
    "        \n",
    "    # å®šä¹‰kerasçš„åµŒå…¥å±‚\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    \n",
    "    # æ„å»ºembeddingå±‚ã€‚\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # å°†åµŒå…¥å±‚çš„æƒé‡è®¾ç½®ä¸ºåµŒå…¥çŸ©é˜µã€‚\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dim: 50\n",
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ„å»ºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    å®ç°Emojify-V2æ¨¡å‹çš„è®¡ç®—å›¾\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        input_shape -- è¾“å…¥çš„ç»´åº¦ï¼Œé€šå¸¸æ˜¯(max_len,)\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹çš„å•è¯ä¸è¯åµŒå…¥çš„æ˜ å°„ã€‚\n",
    "        word_to_index -- å­—å…¸ç±»å‹çš„å•è¯åˆ°è¯æ±‡è¡¨ï¼ˆ400,001ä¸ªå•è¯ï¼‰çš„ç´¢å¼•çš„æ˜ å°„ã€‚\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        model -- Kerasæ¨¡å‹å®ä½“\n",
    "    \"\"\"\n",
    "    # å®šä¹‰sentence_indicesä¸ºè®¡ç®—å›¾çš„è¾“å…¥ï¼Œç»´åº¦ä¸º(input_shape,)ï¼Œç±»å‹ä¸ºdtype 'int32' \n",
    "    sentence_indices = Input(input_shape, dtype=\"int32\")\n",
    "    \n",
    "    # åˆ›å»ºembeddingå±‚\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # é€šè¿‡åµŒå…¥å±‚ä¼ æ’­sentence_indicesï¼Œä½ ä¼šå¾—åˆ°åµŒå…¥çš„ç»“æœ\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # é€šè¿‡å¸¦æœ‰128ç»´éšè—çŠ¶æ€çš„LSTMå±‚ä¼ æ’­åµŒå…¥\n",
    "    # éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿”å›çš„è¾“å‡ºåº”è¯¥æ˜¯ä¸€æ‰¹åºåˆ—ã€‚\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    # ä½¿ç”¨dropoutï¼Œæ¦‚ç‡ä¸º0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # é€šè¿‡å¦ä¸€ä¸ª128ç»´éšè—çŠ¶æ€çš„LSTMå±‚ä¼ æ’­X\n",
    "    # æ³¨æ„ï¼Œè¿”å›çš„è¾“å‡ºåº”è¯¥æ˜¯å•ä¸ªéšè—çŠ¶æ€ï¼Œè€Œä¸æ˜¯ä¸€ç»„åºåˆ—ã€‚\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    # ä½¿ç”¨dropoutï¼Œæ¦‚ç‡ä¸º0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # é€šè¿‡softmaxæ¿€æ´»çš„Denseå±‚ä¼ æ’­Xï¼Œå¾—åˆ°ä¸€æ‰¹5ç»´å‘é‡ã€‚\n",
    "    X = Dense(5)(X)\n",
    "    # æ·»åŠ softmaxæ¿€æ´»\n",
    "    X = Activation(\"softmax\")(X)\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹å®ä½“\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dim: 50\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Emojify_V2((max_len,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 1.5927 - accuracy: 0.2803\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 733us/step - loss: 1.5197 - accuracy: 0.3409\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 687us/step - loss: 1.4883 - accuracy: 0.3561\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 605us/step - loss: 1.4070 - accuracy: 0.4318\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 582us/step - loss: 1.3157 - accuracy: 0.5227\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 612us/step - loss: 1.2134 - accuracy: 0.6136\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 536us/step - loss: 1.0884 - accuracy: 0.6439\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 514us/step - loss: 0.9875 - accuracy: 0.6591\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 552us/step - loss: 0.8650 - accuracy: 0.6591\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 582us/step - loss: 0.7298 - accuracy: 0.7273\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 680us/step - loss: 0.6314 - accuracy: 0.7727\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 846us/step - loss: 0.6380 - accuracy: 0.7500\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 567us/step - loss: 0.4961 - accuracy: 0.8258\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 650us/step - loss: 0.4601 - accuracy: 0.8561\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 763us/step - loss: 0.4696 - accuracy: 0.8258\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 710us/step - loss: 0.4131 - accuracy: 0.8333\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 635us/step - loss: 0.4636 - accuracy: 0.8712\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 597us/step - loss: 0.3182 - accuracy: 0.9167\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 642us/step - loss: 0.3112 - accuracy: 0.9091\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 695us/step - loss: 0.3443 - accuracy: 0.8712\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 695us/step - loss: 0.3259 - accuracy: 0.9015\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 695us/step - loss: 0.3420 - accuracy: 0.8485\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 665us/step - loss: 0.3031 - accuracy: 0.8788\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 771us/step - loss: 0.3255 - accuracy: 0.8864\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 544us/step - loss: 0.2128 - accuracy: 0.9621\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 650us/step - loss: 0.2071 - accuracy: 0.9318\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 514us/step - loss: 0.2283 - accuracy: 0.9242\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 710us/step - loss: 0.2286 - accuracy: 0.9242\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 521us/step - loss: 0.1726 - accuracy: 0.9318\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 552us/step - loss: 0.1379 - accuracy: 0.9621\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 657us/step - loss: 0.0981 - accuracy: 0.9773\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 597us/step - loss: 0.1124 - accuracy: 0.9470\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 491us/step - loss: 0.0902 - accuracy: 0.9545\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 491us/step - loss: 0.0585 - accuracy: 0.9924\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 544us/step - loss: 0.0986 - accuracy: 0.9773\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 763us/step - loss: 0.0651 - accuracy: 0.9621\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 589us/step - loss: 0.0514 - accuracy: 0.9848\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 574us/step - loss: 0.0529 - accuracy: 0.9848\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 461us/step - loss: 0.0527 - accuracy: 0.9848\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 559us/step - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 733us/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 771us/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 688us/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 672us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 536us/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 491us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 514us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 544us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 687us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 642us/step - loss: 0.0052 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2b619358fd0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, max_len)\n",
    "Y_train_oh = emo_utils.convert_to_one_hot(Y_train, C=5)\n",
    "\n",
    "model.fit(X_train_indices, Y_train_oh, epochs=50, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æµ‹è¯•é›†è¡¨ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 2ms/step\n",
      "Test accuracy =  0.8571428656578064\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = emo_utils.convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜„   é¢„æµ‹ç»“æœï¼š she got me a nice present\tâ¤ï¸\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜   é¢„æµ‹ç»“æœï¼š work is hard\tğŸ˜„\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜   é¢„æµ‹ç»“æœï¼š This girl is messing with me\tâ¤ï¸\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šâ¤ï¸   é¢„æµ‹ç»“æœï¼š I love taking breaks\tğŸ˜\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜„   é¢„æµ‹ç»“æœï¼š you brighten my day\tâ¤ï¸\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜   é¢„æµ‹ç»“æœï¼š she is a bully\tâ¤ï¸\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜   é¢„æµ‹ç»“æœï¼š go away\tâš¾\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ´   é¢„æµ‹ç»“æœï¼š I did not have breakfast â¤ï¸\n"
     ]
    }
   ],
   "source": [
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('æ­£ç¡®è¡¨æƒ…ï¼š'+ emo_utils.label_to_emoji(Y_test[i]) + '   é¢„æµ‹ç»“æœï¼š '+ X_test[i] + emo_utils.label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you son of a bitch â¤ï¸\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['you son of a bitch'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  emo_utils.label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
